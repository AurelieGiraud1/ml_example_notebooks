{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Yacht Resistance with Neural Networks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook is a simple demonstration of how to use scikit-learn to build a Neural Network for regression. It uses a dataset of 308 experiments and their various attributes. The goal is to predict the residuary resistance per unit weight of displacement based upon the attributes.\n",
    "\n",
    "## The Data\n",
    "\n",
    "The data has been taken from [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml) and the raw data and information can be found [here](https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics). \n",
    "\n",
    "The columns are as follow:\n",
    "\n",
    "1. Longitudinal position of the center of buoyancy, adimensional.\n",
    "2. Prismatic coefficient, adimensional.\n",
    "3. Length-displacement ratio, adimensional.\n",
    "4. Beam-draught ratio, adimensional.\n",
    "5. Length-beam ratio, adimensional.\n",
    "6. Froude number, adimensional.\n",
    "7. Residuary resistance per unit weight of displacement, adimensional. \n",
    "\n",
    "Where column 7 is the target variable we are looking to predict.\n",
    "\n",
    "We import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the data we've saved, passing the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yacht = pd.read_csv(\"data/yacht_hydrodynamics.csv\", names=[\"longitudinal_pos\", \"presmatic_coef\", \"length_disp\", \"beam-draught_rt\", \n",
    "                                                           \"length-beam_rt\", \"froude_num\", \"resid_resist\"], sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the first few rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitudinal_pos</th>\n",
       "      <th>presmatic_coef</th>\n",
       "      <th>length_disp</th>\n",
       "      <th>beam-draught_rt</th>\n",
       "      <th>length-beam_rt</th>\n",
       "      <th>froude_num</th>\n",
       "      <th>resid_resist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.568</td>\n",
       "      <td>4.78</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.568</td>\n",
       "      <td>4.78</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.568</td>\n",
       "      <td>4.78</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.568</td>\n",
       "      <td>4.78</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.568</td>\n",
       "      <td>4.78</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.225</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitudinal_pos  presmatic_coef  length_disp  beam-draught_rt  \\\n",
       "0              -2.3           0.568         4.78             3.99   \n",
       "1              -2.3           0.568         4.78             3.99   \n",
       "2              -2.3           0.568         4.78             3.99   \n",
       "3              -2.3           0.568         4.78             3.99   \n",
       "4              -2.3           0.568         4.78             3.99   \n",
       "\n",
       "   length-beam_rt  froude_num  resid_resist  \n",
       "0            3.17       0.125          0.11  \n",
       "1            3.17       0.150          0.27  \n",
       "2            3.17       0.175          0.47  \n",
       "3            3.17       0.200          0.78  \n",
       "4            3.17       0.225          1.18  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yacht.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly check if we have any null values in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yacht.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do! Let's use the \"describe\" method to find them, amongst other interesting information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitudinal_pos</th>\n",
       "      <th>presmatic_coef</th>\n",
       "      <th>length_disp</th>\n",
       "      <th>beam-draught_rt</th>\n",
       "      <th>length-beam_rt</th>\n",
       "      <th>froude_num</th>\n",
       "      <th>resid_resist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>308.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>308.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.381818</td>\n",
       "      <td>0.563944</td>\n",
       "      <td>4.008182</td>\n",
       "      <td>4.096364</td>\n",
       "      <td>3.341364</td>\n",
       "      <td>0.824318</td>\n",
       "      <td>8.476461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.513219</td>\n",
       "      <td>0.022947</td>\n",
       "      <td>1.643974</td>\n",
       "      <td>0.653655</td>\n",
       "      <td>0.391571</td>\n",
       "      <td>1.146200</td>\n",
       "      <td>14.052367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>2.810000</td>\n",
       "      <td>2.730000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-2.400000</td>\n",
       "      <td>0.546000</td>\n",
       "      <td>4.340000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.367500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.300000</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>4.780000</td>\n",
       "      <td>3.990000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>1.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-2.300000</td>\n",
       "      <td>0.574000</td>\n",
       "      <td>4.780000</td>\n",
       "      <td>4.770000</td>\n",
       "      <td>3.530000</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>8.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>5.140000</td>\n",
       "      <td>5.350000</td>\n",
       "      <td>4.240000</td>\n",
       "      <td>3.510000</td>\n",
       "      <td>62.420000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitudinal_pos  presmatic_coef  length_disp  beam-draught_rt  \\\n",
       "count        308.000000      252.000000   308.000000       308.000000   \n",
       "mean          -2.381818        0.563944     4.008182         4.096364   \n",
       "std            1.513219        0.022947     1.643974         0.653655   \n",
       "min           -5.000000        0.530000     0.530000         2.810000   \n",
       "25%           -2.400000        0.546000     4.340000         3.750000   \n",
       "50%           -2.300000        0.565000     4.780000         3.990000   \n",
       "75%           -2.300000        0.574000     4.780000         4.770000   \n",
       "max            0.000000        0.600000     5.140000         5.350000   \n",
       "\n",
       "       length-beam_rt  froude_num  resid_resist  \n",
       "count      308.000000  308.000000    308.000000  \n",
       "mean         3.341364    0.824318      8.476461  \n",
       "std          0.391571    1.146200     14.052367  \n",
       "min          2.730000    0.125000      0.010000  \n",
       "25%          3.150000    0.225000      0.367500  \n",
       "50%          3.170000    0.325000      1.790000  \n",
       "75%          3.530000    0.425000      8.092500  \n",
       "max          4.240000    3.510000     62.420000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yacht.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... the column *presmatic_coef* has 56 missing values... we can deal with this in a few different ways. The simpliest solution is to remove them, though we lose many examples in doing so. Alternatively, we could impute the values, replacing the NaN values with an average (mean or median). For the purpose of this simple notebook, we will simply remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yacht = yacht.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Data\n",
    "\n",
    "The purpose of splitting the data is to be able to assess the quality of a predictive model when it is used on unseen data. When training, you will try to build a model that fits to the data as closely as possible, to be able to most accurately make a prediction. However, without a test set you run the risk of overfitting - the model works very well for the data it has seen but not for new data.\n",
    "\n",
    "The split ratio is often debated and in practice you might split your data into three sets: train, validation and test. You would use the training data to understand which classifier you wish to use; the validation set to test on whilst tweaking parameters; and the test set to get an understanding of how your final model would work in practice. Furthermore, there are techniques such as K-Fold cross validation that also help to reduce bias.\n",
    "\n",
    "For the purpose of this demonstration, we will only be randomly splitting our data into test and train, with a 80/20 split.\n",
    "\n",
    "We import the required library from scikit-learn, [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish for all features to be used for training, therefore we are taking all columns except \"class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = yacht.drop([\"resid_resist\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column \"class\" is our target variable, we set y as this column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = yacht[\"resid_resist\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the *train_test_split* function to create the appropriate train and test data for our features (\"X_train\" and \"X_test\" respectively) and target data (\"Y_train\" and \"Y_test\"). We are specifying our test data to be 20% of the total data. We are also providing a seed to be able to reproduce this split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of examples we have in each of our train and test data sets using \"shape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardisation\n",
    "\n",
    "All features are numeric so we do not need to worry about converting categorical data with techniques such as one-hot encoding. However, we will demonstrate how to standardise our data. Although not necessary with neural networks, it often makes training more efficient. Standardisation rescales our attributes so they have a mean of 0 and standard deviation of 1. It assumes that the distribution is Gaussian (it works better if it is), alternatively normalisation can be used to rescale between the range of 0 and 1\n",
    "\n",
    "We use scikit-learn's [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the scaler, leaving parameters as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the scaler passing the training data but also request it transforms the data and returns it to a variable named \"train_scaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then transform our test data with the same fitted scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Neural networks can learn complex patterns using layers of neurons which mathematically transform the data. The layers between the input and output are referred to as “hidden layers”. A neural network can learn relationships between the features that other algorithms cannot easily discover.\n",
    "\n",
    "We are using scikit-learn's [MLP Regressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) which is a simple type of neural network called a multi-layer perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MLPRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train it with our scaled training data and target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/staceyro/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We wish to understand how good our model is; there are a few different metrics we can use. We will evaluate mean squared error (MSE) and mean absolute error (MAE)\n",
    "\n",
    "We import [scikit-learn's mean squared error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) and [sckit-learn's mean absolute error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the errors for our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_train, model.predict(train_scaled))\n",
    "mae = mean_absolute_error(y_train, model.predict(train_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse =  175.37535286838846  & mae =  7.565456959439616  & rmse =  13.242935961046873\n"
     ]
    }
   ],
   "source": [
    "print(\"mse = \",mse,\" & mae = \",mae,\" & rmse = \", sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easier metric to understand is the mean absolute error, this means that on average our prediction was 7.6 away from the true prediction. Mean squared error, and consequently root mean squared error (RMSE), results in predictions further and further from the true value are punished more.\n",
    "\n",
    "We can calculate the same on the test data to understand how we the model is generalised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse =  50.308763368705925  & mae =  5.139610713336014  & rmse =  7.092867076768457\n"
     ]
    }
   ],
   "source": [
    "test_mse = mean_squared_error(y_test, model.predict(test_scaled))\n",
    "test_mae = mean_absolute_error(y_test, model.predict(test_scaled))\n",
    "print(\"mse = \",test_mse,\" & mae = \",test_mae,\" & rmse = \", sqrt(test_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are actually seeing better results on our test data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Parameters\n",
    "\n",
    "More information on Neural Networks can be found in the scikit-learn documentation [here](http://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
    "\n",
    "There are a number of parameters that can be tuned that should be explored when trying to improve Neural Network models. A common approach is to test many different parameters, building multiple models and testing their accuracy to find the best combination.\n",
    "\n",
    "### Parameters\n",
    "For Multi-layer Perceptron (MLP) regressors, the [scikit-learn documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) provides parameters that can be passed by the user; changing these are likely to have an impact on the performance of the model. \n",
    "\n",
    "Here is high-level information on the parameters, the documentation has more details:\n",
    "- hidden_layer_sizes : default (100,)\n",
    "    - The ith element represents the number of neurons in the ith hidden layer.\n",
    "\n",
    "- activation : default ‘relu’\n",
    "    - Activation function for the hidden layer.\n",
    "\n",
    "- solver : default ‘adam’\n",
    "    - The solver for weight optimization.\n",
    "\n",
    "- alpha : default 0.0001\n",
    "    - L2 penalty (regularization term) parameter.\n",
    "\n",
    "- batch_size : default ‘auto’\n",
    "    - Size of minibatches for stochastic optimizers. If the solver is ‘lbfgs’, the classifier will not use minibatch. When set to “auto”, batch_size=min(200, n_samples)\n",
    "\n",
    "- learning_rate : default ‘constant’\n",
    "    - Learning rate schedule for weight updates.\n",
    "\n",
    "- learning_rate_init : default 0.001\n",
    "    - The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’.\n",
    "\n",
    "- power_t : default 0.5\n",
    "    - The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to ‘invscaling’. Only used when solver=’sgd’.\n",
    "\n",
    "- max_iter : default 200\n",
    "    - Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations. For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.\n",
    "\n",
    "- shuffle : default True\n",
    "    - Whether to shuffle samples in each iteration. Only used when solver=’sgd’ or ‘adam’.\n",
    "\n",
    "- random_state : default None\n",
    "    - If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "\n",
    "- tol : default 1e-4\n",
    "    - Tolerance for the optimization. When the loss or score is not improving by at least tol for two consecutive iterations, unless learning_rate is set to ‘adaptive’, convergence is considered to be reached and training stops.\n",
    "\n",
    "- verbose : default False\n",
    "    - Whether to print progress messages to stdout.\n",
    "\n",
    "- warm_start : default False\n",
    "    - When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.\n",
    "\n",
    "- momentum : default 0.9\n",
    "    - Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=’sgd’.\n",
    "\n",
    "- nesterovs_momentum : default True\n",
    "    - Whether to use Nesterov’s momentum. Only used when solver=’sgd’ and momentum > 0.\n",
    "\n",
    "- early_stopping : default False\n",
    "    - Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for two consecutive epochs. Only effective when solver=’sgd’ or ‘adam’\n",
    "\n",
    "- validation_fraction : default 0.1\n",
    "    - The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True\n",
    "\n",
    "- beta_1 : default 0.9\n",
    "    - Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver=’adam’\n",
    "\n",
    "- beta_2 : default 0.999\n",
    "    - Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver=’adam’\n",
    "\n",
    "- epsilon : default 1e-8\n",
    "    - Value for numerical stability in adam. Only used when solver=’adam’\n",
    "\n",
    "### Parameter Search\n",
    "\n",
    "To search for the best hyper-parameters for your algorithm and data, grid search cross validation is commonly used. Alternatively, there is a randomized search approach and this is often preferred for deep learning (working with neural networks with many layers). The [scikit-learn documentation](http://scikit-learn.org/stable/modules/grid_search.html) provides more thorough information on how to use these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Citation\n",
    "\n",
    "Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
